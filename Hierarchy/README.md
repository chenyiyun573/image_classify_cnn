20230416 19:05 Complete train5.py. When writing train5.py, I create a new NetworkTree.py classes to store 
Information of each network to avoid complex hierarchy preprocessing for training.


20230414 16:15 Try to organize train4.py, also I want to change its dataset filtering ways. Create train5.py based on train4.py


20230414 10:57 Let me try another solution, that is `mp.set_start_method('spawn')` This will change the way processes are spawned, which may help avoid deadlocks. Note that setting the start method should be done only once per script and should be called before using any other multi-processing functions. It works, however, some error happens like `AttributeError: Can't pickle local object 'root_target_transform_factory.<locals>.root_target_transform' ` , also the speed of such a multiple processes training is very slow. I guess its due to the lack of good scheduler. Although GPU can work together, the dataloader and some functions can only be occupied by only one process.


20230413 18:20 I copy train3.py to create train4, this time I want to solve the deadlock problem. GPT provides two ways one is that 1. To resolve the issue, you can try setting num_workers to 0 in your DataLoader. Another one is that 2. Another potential solution is to use the spawn start method for multi-processing. You can set this start method by adding the following line at the beginning of your script.
Set num_works to 0 seems do not work since I have waitted for a long time after processes all created. 
Meet deadlock again, after ctrl-C to exit, remember `ps aux | grep train4.py` and `kill 49972 49973 49974 49975` to kill the processes.


Next to do for train3.py
    solve the deadlock problem so that networks can be trained at the sametime on multiple GPUs
    write the inference part of code so that I can get the accuracy result by these networks - done
    organize the code in a good format so that its reader friendly.


20230413 18:06 the following is result from train3.py
```
gpu_id 3 network Root training started at current time 2023-04-13 08:50:07
number classes3 in Root
[gpu id: 3  epoch1, Root] loss: 0.7599945313930512
[gpu id: 3  epoch2, Root] loss: 0.6398491742908955
[gpu id: 3  epoch3, Root] loss: 0.59017669698596
[gpu id: 3  epoch4, Root] loss: 0.547644259661436
[gpu id: 3  epoch5, Root] loss: 0.5165673054754734
[gpu id: 3  epoch6, Root] loss: 0.4844599612355232
[gpu id: 3  epoch7, Root] loss: 0.4581007656157017
[gpu id: 3  epoch8, Root] loss: 0.43320960925519464
[gpu id: 3  epoch9, Root] loss: 0.4074602149873972
[gpu id: 3  epoch10, Root] loss: 0.3832511046677828
Finished training Root
 gpu_id 3 network Root training finished at current time 2023-04-13 08:54:03
models saved.
```


20230413 15:50 based on train2.py, train3.py add lines to store the model I trained. In train_accuracy.py, use train_set to test result from train3.py's models. Notice that the train_set was filtered with only 59+59+72 problems.


20230413 13:23 I write a train_accurarcy.py to get the final acccurarcy of the whole hierarchy, but one thing need to be noticed
that the testing may use trainset 200 classes, the training only have 59+59+72 totally 190 class ids. So, I add some filtering code 
for the dataset.


train.py cannot work well.
train2.py works well. Now in train2.py. When I try to use multiple process to let the network be trained seperately on different GPUs, I seems encounter a deadlock problem. 

these following is generated by GPT4 when I describe my problem:
    Processes are taking a long time to execute: Depending on the size and complexity of your training dataset and the neural network, the training process could take a long time to complete. In this case, the join() method would block the main program from continuing until all the processes have finished.

    Deadlock or synchronization issues: If your processes are sharing resources or waiting for each other, it's possible that a deadlock or synchronization issue is causing your program to hang. You'll need to review your multiprocessing code and shared resources to identify and fix any potential issues.

    Print statements not appearing: When using multiprocessing, print statements within a process may not always appear on the terminal immediately. You can try to add sys.stdout.flush() after the print statements to force the output to be displayed immediately. You need to import the sys module first by adding import sys at the beginning of your code.

result of train2.py:
```
{'n01443537': 0, 'n01644900': 1, 'n01629819': 2, 'n01641577': 3, 'n01698640': 4, 'n01742172': 5, 'n01768244': 6, 'n01770393': 7, 'n01774384': 8, 'n01774750': 9, 'n01784675': 10, 'n02231487': 11, 'n02233338': 12, 'n02236044': 13, 'n02268443': 14, 'n02279972': 15, 'n02281406': 16, 'n02165456': 17, 'n02190166': 18, 'n02206856': 19, 'n02226429': 20, 'n01855672': 21, 'n02002724': 22, 'n02056570': 23, 'n02058221': 24, 'n02074367': 25, 'n02085620': 26, 'n02094433': 27, 'n02099601': 28, 'n02099712': 29, 'n02106662': 30, 'n02123045': 31, 'n02123394': 32, 'n02124075': 33, 'n02125311': 34, 'n02129165': 35, 'n02132136': 36, 'n02364673': 37, 'n02395406': 38, 'n02403003': 39, 'n02410509': 40, 'n02415577': 41, 'n02423022': 42, 'n02437312': 43, 'n02480495': 44, 'n02481823': 45, 'n02486410': 46, 'n02504458': 47, 'n01882714': 48, 'n02509815': 49, 'n01910747': 50, 'n01944390': 51, 'n01945685': 52, 'n01950731': 53, 'n01983481': 54, 'n01984695': 55, 'n02321529': 56, 'n01917289': 57, 'n02113799': 58}
Number of labels: 59
Labels:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58}
Number of labels: 59
{'n02666196': 0, 'n02699494': 1, 'n02892201': 2, 'n02769748': 3, 'n02795169': 4, 'n02815834': 5, 'n03937543': 6, 'n03983396': 7, 'n02909870': 8, 'n03400231': 9, 'n04070727': 10, 'n04067472': 11, 'n04118538': 12, 'n04023962': 13, 'n02841315': 14, 'n02843684': 15, 'n03902125': 16, 'n03976657': 17, 'n04149813': 18, 'n02808440': 19, 'n02906734': 20, 'n02948072': 21, 'n03444034': 22, 'n04596742': 23, 'n04597913': 24, 'n04398044': 25, 'n04560804': 26, 'n07579787': 27, 'n04456115': 28, 'n04399382': 29, 'n04146614': 30, 'n04465501': 31, 'n03796401': 32, 'n03977966': 33, 'n04285008': 34, 'n04532106': 35, 'n04507155': 36, 'n03891332': 37, 'n03992509': 38, 'n04008634': 39, 'n04099969': 40, 'n04251144': 41, 'n04265275': 42, 'n04275548': 43, 'n03838899': 44, 'n03854065': 45, 'n03804744': 46, 'n03814639': 47, 'n03970156': 48, 'n04376876': 49, 'n04074963': 50, 'n04179913': 51, 'n03837869': 52, 'n03930313': 53, 'n04311004': 54, 'n04366367': 55, 'n02823428': 56, 'n02837789': 57, 'n02883205': 58, 'n03255030': 59, 'n03393912': 60, 'n04328186': 61, 'n04417672': 62, 'n04486054': 63, 'n04487081': 64, 'n04501370': 65, 'n04532670': 66, 'n04540053': 67, 'n04562935': 68}
Number of labels: 69
Labels:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68}
Number of labels: 69
{'n02669723': 0, 'n02730930': 1, 'n04371430': 2, 'n04254777': 3, 'n04356056': 4, 'n02963159': 5, 'n03404251': 6, 'n03617480': 7, 'n03763968': 8, 'n03770439': 9, 'n04133789': 10, 'n03980874': 11, 'n04259630': 12, 'n02917067': 13, 'n03100240': 14, 'n03447447': 15, 'n03662601': 16, 'n03670208': 17, 'n03250847': 18, 'n02977058': 19, 'n02988304': 20, 'n03085013': 21, 'n03584254': 22, 'n02950826': 23, 'n02999410': 24, 'n03126707': 25, 'n03160309': 26, 'n03179701': 27, 'n02927161': 28, 'n03014705': 29, 'n03026506': 30, 'n03042490': 31, 'n03089624': 32, 'n03201208': 33, 'n03355925': 34, 'n03388043': 35, 'n03424325': 36, 'n03544143': 37, 'n03599486': 38, 'n03637318': 39, 'n03649909': 40, 'n03706229': 41, 'n03733131': 42, 'n07583066': 43, 'n07715103': 44, 'n07720875': 45, 'n07734744': 46, 'n07747607': 47, 'n07749582': 48, 'n07753592': 49, 'n07768694': 50, 'n07614500': 51, 'n07615774': 52, 'n07871810': 53, 'n07873807': 54, 'n07875152': 55, 'n07711569': 56, 'n07920052': 57, 'n07695742': 58, 'n09193705': 59, 'n09246464': 60, 'n09256479': 61, 'n09332890': 62, 'n09428293': 63, 'n12267677': 64, 'n06596364': 65, 'n02788148': 66, 'n02791270': 67, 'n02793495': 68, 'n02814533': 69, 'n02814860': 70, 'n02802426': 71}
Number of labels: 72
Labels:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71}
Number of labels: 72
Number of labels: 3
Labels:  {0, 1, 2}
 gpu_id 0 network Animals training started 
number classes59 in Animals
/home/superbench/v-yiyunchen/ytorch-venv/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
[gpu id: 0  epoch1, Animals] loss: 2.9186957941216938
[gpu id: 0  epoch2, Animals] loss: 1.752566316168187
[gpu id: 0  epoch3, Animals] loss: 1.4371485742471986
[gpu id: 0  epoch4, Animals] loss: 1.2647393216521052
[gpu id: 0  epoch5, Animals] loss: 1.1362413337675192
[gpu id: 0  epoch6, Animals] loss: 1.0405699162159936
[gpu id: 0  epoch7, Animals] loss: 0.9506416377374681
[gpu id: 0  epoch8, Animals] loss: 0.8739720168760268
[gpu id: 0  epoch9, Animals] loss: 0.8069856199167542
[gpu id: 0  epoch10, Animals] loss: 0.7386393411684844
Finished training Animals
 gpu_id 0 network Animals training finished 
 gpu_id 1 network Objects training started 
number classes69 in Objects
[gpu id: 1  epoch1, Objects] loss: 3.1652588920316833
[gpu id: 1  epoch2, Objects] loss: 2.012306193337924
[gpu id: 1  epoch3, Objects] loss: 1.6955706088439277
[gpu id: 1  epoch4, Objects] loss: 1.5027029801106107
[gpu id: 1  epoch5, Objects] loss: 1.363100952687471
[gpu id: 1  epoch6, Objects] loss: 1.2485979258150295
[gpu id: 1  epoch7, Objects] loss: 1.1514564946077872
[gpu id: 1  epoch8, Objects] loss: 1.0655845965164294
[gpu id: 1  epoch9, Objects] loss: 0.9841026142023612
[gpu id: 1  epoch10, Objects] loss: 0.9075550188188968
Finished training Objects
 gpu_id 1 network Objects training finished 
 gpu_id 2 network Others training started 
number classes72 in Others
[gpu id: 2  epoch1, Others] loss: 2.809320874677764
[gpu id: 2  epoch2, Others] loss: 1.6552508963478936
[gpu id: 2  epoch3, Others] loss: 1.3834607867730988
[gpu id: 2  epoch4, Others] loss: 1.2178847670555115
[gpu id: 2  epoch5, Others] loss: 1.106861191822423
[gpu id: 2  epoch6, Others] loss: 1.0066789151893722
[gpu id: 2  epoch7, Others] loss: 0.9227573321925269
[gpu id: 2  epoch8, Others] loss: 0.8428832474682066
[gpu id: 2  epoch9, Others] loss: 0.778736192236344
[gpu id: 2  epoch10, Others] loss: 0.7159278976420561
Finished training Others
 gpu_id 2 network Others training finished 
 gpu_id 3 network Root training started 
number classes3 in Root
[gpu id: 3  epoch1, Root] loss: 0.7602542719841003
[gpu id: 3  epoch2, Root] loss: 0.6453871874213218
[gpu id: 3  epoch3, Root] loss: 0.5944919187426567
[gpu id: 3  epoch4, Root] loss: 0.5527971988618374
[gpu id: 3  epoch5, Root] loss: 0.5162440326213836
[gpu id: 3  epoch6, Root] loss: 0.48640381219983103
[gpu id: 3  epoch7, Root] loss: 0.46125548279285433
[gpu id: 3  epoch8, Root] loss: 0.43529402795433997
[gpu id: 3  epoch9, Root] loss: 0.4087060429751873
[gpu id: 3  epoch10, Root] loss: 0.3850900271087885
Finished training Root
 gpu_id 3 network Root training finished
```




